{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4McgZlKYAuIt"
      },
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "This notebook was created as part of the \"Deep learning / VITMMA19\" class at\n",
        "Budapest University of Technology and Economics, Hungary,\n",
        "https://portal.vik.bme.hu/kepzes/targyak/VITMMA19.\n",
        "\n",
        "Any re-use or publication of any part of the notebook is only allowed with the\n",
        "written consent of the authors.\n",
        "\n",
        "2023 (c) András Béres (beres kukac tmit pont bme pont hu)\n",
        "</PRE>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary packages:\n",
        "* PyTorch Lightning: high-level deep learning library over PyTorch.\n",
        "* Weights and Biases: library used for logging metrics and model checkpoints.\n",
        "* Datasets: library for loading datasets from the Hugging Face Hub (https://huggingface.co/datasets)."
      ],
      "metadata": {
        "id": "WWC91jCmhWOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TIOn42aE09N"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning\n",
        "!pip install -q wandb\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the necessary libraries."
      ],
      "metadata": {
        "id": "RlZV7jEzh5WA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGGLR98Qt_ZW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to WandB. Make sure that you have a valid WandB account. Ctrl + click on the link, copy the token from the opened page."
      ],
      "metadata": {
        "id": "dVTddMDnh-zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "wY375kLbY9JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "6jeMzCA3iMZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77ar_-KxtGXJ"
      },
      "outputs": [],
      "source": [
        "# we will use these later for the CNN\n",
        "resolution = 112\n",
        "num_classes = 3\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "num_epochs = 30\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 1e-4\n",
        "width = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pipeline"
      ],
      "metadata": {
        "id": "YrKzzmU-iT-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will be using the https://huggingface.co/datasets/lewtun/dog_food dataset.\n",
        "* What is the difference between dataset and dataloader?\n",
        "* What is the difference between image preprocessing and image augmentations? Look at the transforms. What do we use during training and validation?\n",
        "* Best practices:\n",
        "    * Set `num_workers > 0`. A good starting point is the number of CPU cores on your system.\n",
        "    * If you use a GPU, set `pin_memory = True`. This improves the speed of CPU-to-GPU data transfer.\n",
        "    * If you want to have a consistent batch_size in all iterations, set `drop_last = True`.\n",
        "    * During training, set `shuffle = True`, which shuffles the order of frames in the dataset."
      ],
      "metadata": {
        "id": "WzntXO2MecVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDQOlrgXtg3t"
      },
      "outputs": [],
      "source": [
        "class DogFoodDataModule(pl.LightningDataModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.train_transform = transforms.Compose([\n",
        "            # transforms.Resize(size=resolution, antialias=True),\n",
        "            # transforms.CenterCrop(size=resolution),\n",
        "            transforms.RandomResizedCrop(size=resolution, scale=(0.25, 1.0), antialias=True),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        self.val_transform = transforms.Compose([\n",
        "            transforms.Resize(size=resolution, antialias=True),\n",
        "            transforms.CenterCrop(size=resolution),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def preprocess_train(self, batch):\n",
        "        batch[\"image\"] = [self.train_transform(image) for image in batch[\"image\"]]\n",
        "        return batch\n",
        "\n",
        "    def preprocess_val(self, batch):\n",
        "        batch[\"image\"] = [self.val_transform(image) for image in batch[\"image\"]]\n",
        "        return batch\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = datasets.load_dataset(\n",
        "            \"lewtun/dog_food\", split=\"train\"\n",
        "        ).with_transform(self.preprocess_train)\n",
        "        self.val_dataset = datasets.load_dataset(\n",
        "            \"lewtun/dog_food\", split=\"test\"\n",
        "        ).with_transform(self.preprocess_val)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True,\n",
        "            drop_last=True,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.val_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datamodule = DogFoodDataModule()\n",
        "datamodule.prepare_data()\n",
        "datamodule.setup()"
      ],
      "metadata": {
        "id": "ReRD86goM26B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "* PyTorch uses channels first representation (batch, channels, height, width) vs channels last (batch, height, width, channels) by default. Keep that in mind during visualization, because matplotlib expects channels last format."
      ],
      "metadata": {
        "id": "jHo40fcHjzQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_images(dataset, num_images, num_augmentations):\n",
        "    class_names = [\"chicken\", \"dog\", \"muffin\"]\n",
        "    plt.figure(figsize=(num_augmentations * 2, num_images * 2))\n",
        "    for row in range(num_images):\n",
        "        for column in range(num_augmentations):\n",
        "            image = dataset[row][\"image\"]\n",
        "            label = dataset[row][\"label\"]\n",
        "            plt.subplot(num_images, num_augmentations, num_augmentations * row + column + 1)\n",
        "            plt.imshow(image.cpu().permute(1, 2, 0).numpy())\n",
        "            if column == 0:\n",
        "                plt.title(class_names[label], loc=\"left\")\n",
        "            plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mtPw0uz3OYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_images(datamodule.train_dataset, num_images=4, num_augmentations=5)"
      ],
      "metadata": {
        "id": "p703aeo3U0--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_images(datamodule.val_dataset, num_images=4, num_augmentations=5)"
      ],
      "metadata": {
        "id": "tV7HoZgTU3Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier Model Definition"
      ],
      "metadata": {
        "id": "eHI9Pptqj6Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcxeTGtJDNVc"
      },
      "outputs": [],
      "source": [
        "class ImageClassifier(pl.LightningModule):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pred_labels = self(batch[\"image\"])\n",
        "\n",
        "        loss = F.cross_entropy(pred_labels, batch[\"label\"])\n",
        "        accuracy = self.accuracy(pred_labels, batch[\"label\"])\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_epoch=True)\n",
        "        self.log(\"train_acc\", accuracy, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        pred_labels = self(batch[\"image\"])\n",
        "\n",
        "        loss = F.cross_entropy(pred_labels, batch[\"label\"])\n",
        "        accuracy = self.accuracy(pred_labels, batch[\"label\"])\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", accuracy, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        pred_labels = self(images)\n",
        "\n",
        "        loss = F.cross_entropy(pred_labels, labels)\n",
        "        accuracy = self.accuracy(pred_labels, labels)\n",
        "\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_acc\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        # return torch.optim.AdamW(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        # return torch.optim.AdamW(self.model.classifier[-1].parameters(), lr=learning_rate, weight_decay=weight_decay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network Definiton"
      ],
      "metadata": {
        "id": "q76oLThBklwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Observe our custom CNN.\n",
        "* What do each of the Conv2d parameters mean?\n",
        "* What are the output shapes of each layer?\n",
        "* What is a fully convolutional neural network? What is their advantage?"
      ],
      "metadata": {
        "id": "NsJku9dBe4ZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StuQfSs6b7RD"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(in_channels=3, out_channels=width, kernel_size=3, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(in_channels=width, out_channels=width, kernel_size=3, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(in_channels=width, out_channels=width, kernel_size=3, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(in_channels=width, out_channels=width, kernel_size=3, stride=2, padding=1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.AdaptiveAvgPool2d(output_size=1),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(in_features=width, out_features=num_classes),\n",
        ")\n",
        "\n",
        "# model = torchvision.models.mobilenet_v2(num_classes=num_classes)\n",
        "\n",
        "# model = torchvision.models.mobilenet_v2(num_classes=1000, weights=\"IMAGENET1K_V2\")\n",
        "# model.classifier[-1] = torch.nn.Linear(in_features=1280, out_features=num_classes, bias=True)\n",
        "\n",
        "print(model)\n",
        "print(model(torch.randn(batch_size, 3, resolution, resolution)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best practices:\n",
        "* Use 16-bit floating point mixed precision for >2x faster training and 2x lower GPU memory consumption.\n",
        "* If using a PyTorch 2.x on a modern GPU (Volta or Ampere architecture e.g. V100 or A100), compile your neural network.\n",
        "* For better GPU utilization, use larger batch sizes."
      ],
      "metadata": {
        "id": "OQ5R4FCvfZXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.compile(model)\n",
        "lit_model = ImageClassifier(model)"
      ],
      "metadata": {
        "id": "M9JWvnPKcEz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "* We log our metrics using a `WandbLogger`.\n",
        "* We save our best checkpoints using a `ModelCheckpoint` callback."
      ],
      "metadata": {
        "id": "pRcoGOuOmA4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_logger = pl.loggers.WandbLogger(project=\"dl-practice-4-vision\", log_model=\"all\")\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_acc\", mode=\"max\")\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=num_epochs,\n",
        "    precision=\"16-mixed\",\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")\n",
        "trainer.fit(lit_model, datamodule)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "P8GYMS2LYRga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you stop your training early, run this line manually, to finish the wandb run\n",
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "ICnw93MXVzpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks (during practice, solved together)\n",
        "1. Visualize the augmented/preprocessed training/validation images. What do we see?\n",
        "2. Turn image augmentations off in the training transforms.\n",
        "3. Inspect the baseline CNN. What are the output shapes after each layer?\n",
        "4. Run the baseline training.\n",
        "5. Remove all ReLUs from our convolutional network. What do we expect? What do we see?\n",
        "6. Replace the CNN with a MobilenetV2 network. How does it perform (train vs validation)?\n",
        "7. Add regularization to the training (turn on image augmentations, add weight decay, by replacing Adam with AdamW).\n",
        "8. Load ImageNet-pretrained weights into the MobileNetV2 network and finetune it.\n",
        "* **Warning 1**: ImageNet has 1000 classes, we only have 3. How does this affect the final layer of the network, what can we do?\n",
        "* **Warning 2**: When using pretrained weights, we need to make sure that we use the same preprocessing as during pretraining. Turn pixel value normalization on in the transforms.\n",
        "9. Try finetuning only the final classification layer of the network. How does this affect performance? What can be the advantages?"
      ],
      "metadata": {
        "id": "dV3HBHIKa1pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks (homework)\n",
        "Your tasks are the following:\n",
        "1. Add some other type of image augmentation to the training pipeline. Check out the list of available transforms here: https://pytorch.org/vision/main/transforms.html. Visualize the training images, save the generated figure.\n",
        "2. Add batch normalization layers (2D) to our custom CNN, after each ReLU. Keep the network width at 64. What is the exact number of parameters in the original network? print it out! What is the exact number of trainable parameters in the new network? print it out!\n",
        "3. Add a confusion matrix to your metrics (https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html). Run an evaluation, and visualize the confusion matrix.\n"
      ],
      "metadata": {
        "id": "N0gHW2U5dtCw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_K7skZHANltR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2rc1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}